# ğŸŒ CareerFlow AI â€” Global News â†’ RAG Pipeline  
### *Real-time automated knowledge engine powering CareerFlow AI*

---

## ğŸ“Œ Introduction

CareerFlow AI requires **fresh, reliable, up-to-date knowledge** to answer questions across careers, technology, world events, economy, business, education, and emerging trends.

Large Language Models (LLMs) cannot stay up-to-date by themselves.  
So this repository provides a **complete automated Retrieval-Augmented Generation (RAG) pipeline** that:

1. Scrapes global news every hour  
2. Extracts full article content  
3. Filters + deduplicates + cleans  
4. Splits into semantic chunks  
5. Computes embeddings incrementally  
6. Stores final RAG datasets on HuggingFace  
7. Auto-creates new versions when size > 100 MB  
8. Powers CareerFlowâ€™s retrieval engine + LLM reasoning  

This README explains **EVERYTHING** about the system architecture, workflow design, datasets, API endpoints, and complete logic behind the entire repo.

---

# ğŸ§  What This Repository Provides

### âœ” Fully autonomous RAG pipeline  
### âœ” Hourly GitHub Actions automation  
### âœ” Trusted multi-source news ingestion  
### âœ” Clean structured dataset output  
### âœ” Incremental embeddings  
### âœ” HuggingFace auto-upload  
### âœ” Multi-version dataset rotation  
### âœ” Retrieval API + LLM answering  

This README documents everything end-to-end.

---

# ğŸ”¥ Why this matters

CareerFlow AI aims to become a **Perplexity-level assistant**, capable of:

- Understanding latest career trends  
- Summarizing fresh global news  
- Giving guidance with updated industry insights  
- Using RAG to avoid hallucinations  
- Staying current **every single hour**  

This pipeline ensures the AI is ALWAYS up-to-date.

---

# ğŸ— System Architecture Overview

```
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  Trusted Global News Sources â”‚
 â”‚  (RSS + HTML extraction)     â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ raw news
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚      Scraper (scrape.py)     â”‚
 â”‚ Extracts + normalizes text    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ raw_news.jsonl
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚     Filter Engine (filter.py)â”‚
 â”‚ Dedupes + cleans + validates â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ filtered_news.jsonl
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Chunk Builder + Embedder     â”‚
 â”‚  dataset_builder.py          â”‚
 â”‚ (incremental embeddings)     â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ rag_docs + embeddings
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   push_to_hf.py (auto-upload)â”‚
 â”‚ Decides folder:              â”‚
 â”‚  rag_storage or rag_storage_v2, v3â€¦ â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ HF upload
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ HuggingFace Dataset Repo     â”‚
 â”‚   /rag_storage               â”‚
 â”‚   /rag_storage_v2            â”‚
 â”‚   /rag_storage_v3            â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ“¦ HuggingFace Dataset Explained

Dataset location:

https://huggingface.co/Sachin21112004/carrerflow-ai/tree/main/rag_storage

The pipeline auto-manages:

- rag_storage/  
- rag_storage_v2/  
- rag_storage_v3/  
- â€¦ (new versions when size > 100 MB)

Each folder contains:

### rag_docs.jsonl
Each line is a chunk:

```
{
  "doc_id": "bbc_2025-12-09_34_chunk3",
  "url": "...",
  "title": "...",
  "published": "2025-12-09T12:34:00",
  "source": "BBC",
  "text": "chunk of article..."
}
```

### embeddings.npy  
A float32 array of shape:

```
(num_chunks, embedding_dim)
```

### emb_ids.jsonl  
Maps each embedding row to a chunk:

```
{"doc_id": "bbc_2025-12-09_34_chunk3"}
```

---

# ğŸ“ Full Repository Structure (Explained)

```
careerflow-pipeline/
â”‚
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ scrape.py                 â†’ Fetches news from RSS feeds
â”‚   â”œâ”€â”€ filter.py                 â†’ Removes duplicates, bad content
â”‚   â”œâ”€â”€ dataset_builder.py        â†’ Chunking + incremental embeddings
â”‚   â”œâ”€â”€ push_to_hf.py             â†’ Uploads to HuggingFace (NO HF_REPO secret)
â”‚   â”œâ”€â”€ raw_news.jsonl            â†’ Generated automatically
â”‚   â”œâ”€â”€ filtered_news.jsonl       â†’ Cleaned articles
â”‚   â”œâ”€â”€ rag_docs.jsonl            â†’ Final chunked dataset
â”‚   â”œâ”€â”€ embeddings.npy            â†’ Embeddings file
â”‚   â””â”€â”€ emb_ids.jsonl             â†’ Maps doc_ids â†’ embedding indices
â”‚
â”œâ”€â”€ api/
â”‚   â””â”€â”€ server.py                 â†’ Retrieval API + LLM answering
â”‚
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ news.yml                  â†’ Hourly pipeline workflow
â”‚
â”œâ”€â”€ requirements.txt              â†’ All dependencies
â””â”€â”€ README.md                     â†’ This file
```

---

# ğŸ§¾ Detailed Breakdown of Every Component

---

## 1ï¸âƒ£ scrape.py â€” Global News Scraper

Responsibilities:

- Load RSS feed list  
- Fetch each article  
- Extract readable content  
- Normalize HTML â†’ clean text  
- Store results in `raw_news.jsonl`

Each line stored as:

```
{
  "title": "...",
  "url": "...",
  "published": "...",
  "source": "...",
  "text": "full extracted article..."
}
```

---

## 2ï¸âƒ£ filter.py â€” Cleaning + Deduplication

Uses SQLite DB `seen.db` to track previously processed URLs.

Removes:

- Duplicate articles  
- Extremely short posts  
- Sponsored/ads  
- Non-English or low-quality text  

Output: `filtered_news.jsonl`.

---

## 3ï¸âƒ£ dataset_builder.py â€” RAG Chunking + Incremental Embeddings

### Tasks:
- Split each article into chunks (300â€“500 characters)  
- Assign unique doc_ids  
- Check which chunks are **new**  
- Embed ONLY new chunks  
- Append new embeddings to `embeddings.npy`  
- Append mapping to `emb_ids.jsonl`  

### Why incremental?
Reduces embedding time from minutes â†’ seconds.

---

## 4ï¸âƒ£ push_to_hf.py â€” Upload to HuggingFace (Auto-Versioning)

### Logic:
1. Check repo size using HuggingFace Hub API  
2. If size < 100 MB:
   - Upload to `rag_storage/`
3. If size >= 100 MB:
   - Create or continue `rag_storage_v2/`, `rag_storage_v3/`, etc.
4. Upload:  
   - rag_docs.jsonl  
   - embeddings.npy  
   - emb_ids.jsonl  

### Important:
No `HF_REPO` secret is used â€” repo name is hardcoded inside script.

You only need:

- **HF_TOKEN** as GitHub secret

---

# â° Hourly GitHub Actions Workflow (news.yml)

This workflow:

- Runs every hour  
- Installs dependencies  
- Executes pipeline  
- Uploads to HuggingFace  

Core steps:

```
checkout repo
setup python
install requirements
python scrape.py
python filter.py
python dataset_builder.py
python push_to_hf.py
```

---

# ğŸ”¥ Retrieval API (api/server.py)

Available endpoints:

### `/retrieve`
Returns top-k similar documents.

Request:

```
{
  "query": "What happened in AI this week?",
  "top_k": 5
}
```

Response:

```
[
  { "doc_id": "...", "text": "...", "score": 0.87 },
  ...
]
```

---

### `/ask`
Combines retrieval + LLM:

- Retrieves context  
- Builds augmented prompt  
- Calls:
  1. Bytez AI  
  2. Gemini  
  3. HuggingFace Inference  

Whichever works first.

---

# ğŸ›  Requirements

Main dependencies:

- sentence-transformers  
- numpy  
- huggingface_hub  
- fastapi  
- uvicorn  
- beautifulsoup4  
- readability-lxml  

---

# ğŸ§ª Running Everything Locally

1 â€” Install:

    pip install -r requirements.txt

2 â€” Scrape:

    python pipeline/scrape.py

3 â€” Filter:

    python pipeline/filter.py

4 â€” Build dataset:

    COMPUTE_EMBEDDINGS=1 python pipeline/dataset_builder.py

5 â€” Push manually (optional):

    export HF_TOKEN=hf_xxx
    python pipeline/push_to_hf.py

6 â€” Start API:

    uvicorn api.server:app --reload

---

# âš ï¸ Troubleshooting

### 1. "Repo Not Found"
Your HF_TOKEN does not have write permissions.

Solution:  
Go to â†’ HuggingFace â†’ Settings â†’ Access Tokens â†’ Create token (write).

---

### 2. "Cannot import huggingface_hub module"
Upgrade:

    pip install --upgrade huggingface_hub

---

### 3. "Nothing uploaded"
Make sure new chunks were generated.

---

# ğŸš€ Future Improvements

- Add FAISS for million-scale retrieval  
- Add embeddings quantization  
- Add source reputation scoring  
- Add additional non-news sources  
- Auto-summarization storage  
- Multi-language support  

---

# â¤ï¸ Credits

Designed with care by **Sachin Rao (CareerFlow AI)**  


This system is now production-ready and scales automatically.

