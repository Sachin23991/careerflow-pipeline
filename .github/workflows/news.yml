name: Update Career Dataset

on:
  schedule:
    - cron: "0 * * * *"
  workflow_dispatch:

permissions:
  contents: write
       # Manual trigger

jobs:
  pipeline:
    runs-on: ubuntu-latest

    steps:
    # -------------------------------
    # 1. Checkout repository
    # -------------------------------
    - name: Checkout repo
      uses: actions/checkout@v3

    # -------------------------------
    # 2. Setup Python
    # -------------------------------
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    # -------------------------------
    # 3. Install Dependencies
    # -------------------------------
    - name: Install dependencies
      run: |
        pip install requests feedparser huggingface_hub transformers beautifulsoup4 lxml

    # -------------------------------
    # 4. Run Pipeline Scripts
    # -------------------------------
    - name: Run scraping + filtering + dataset builder + upload
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        python pipeline/scrape.py
        python pipeline/filter.py
        python pipeline/dataset_builder.py
        python pipeline/push_to_hf.py

    # -------------------------------
    # 5. Auto Commit any updates
    # -------------------------------
    - name: Auto Commit Changes
      uses: stefanzweifel/git-auto-commit-action@v4
      with:
        commit_message: "Auto update: New scraped career data"
