name: Update Career Dataset

on:
  schedule:
    - cron: "0 * * * *"   # Run every hour
  workflow_dispatch:

permissions:
  contents: write

jobs:
  pipeline:
    runs-on: ubuntu-latest

    steps:
    # -------------------------------
    # 1. Checkout repository
    # -------------------------------
    # We fetch depth 0 to ensure we have the history if needed, 
    # though primarily we just need the existing 'train.jsonl'
    - name: Checkout repo
      uses: actions/checkout@v3
      with:
        fetch-depth: 0

    # -------------------------------
    # 2. Setup Python
    # -------------------------------
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    # -------------------------------
    # 3. Install Dependencies
    # -------------------------------
    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install requests feedparser huggingface_hub transformers beautifulsoup4 lxml

    # -------------------------------
    # 4. Run scraping + filtering + dataset builder
    # -------------------------------
    # This generates a FRESH "pipeline/train.jsonl" (contains ONLY data scraped right now)
    - name: Run scraping + filtering + dataset builder
      run: |
        set -e
        python pipeline/scrape.py
        python pipeline/filter.py
        python pipeline/dataset_builder.py

    # --------------------------------------------------------
    # 5. Merge or Overwrite FINAL train.jsonl
    # --------------------------------------------------------
    - name: Merge or Overwrite train.jsonl
      run: |
        set -e
        
        # Get hour in IST (Asia/Kolkata)
        HOUR_IN_IST=$(TZ='Asia/Kolkata' date +%H)
        echo "üïí Current hour in IST: $HOUR_IN_IST"

        # LOGIC:
        # If Midnight (00): We REPLACE the main file with the fresh scrape. (Resets file size)
        # If Other Hour: We RUN MERGE script to add fresh scrape to existing main file.
        
        if [ "$HOUR_IN_IST" = "00" ]; then
          echo "üåô Midnight (IST) detected."
          echo "üîÑ ACTION: Overwriting train.jsonl (Daily Reset)"
          # Overwrite main file with the fresh batch only.
          # This keeps the file from exceeding GitHub's 100MB limit over time.
          cp pipeline/train.jsonl train.jsonl
        else
          echo "‚è≥ Hourly update detected."
          echo "‚ûï ACTION: Appending new unique items to existing train.jsonl"
          # Appends 'pipeline/train.jsonl' content to 'train.jsonl' (handling duplicates)
          python pipeline/dataset_merge.py
        fi

    # -------------------------------
    # 6. Upload final dataset to HuggingFace
    # -------------------------------
    # This runs AFTER the file has been either appended or overwritten.
    # It will upload the current state of 'train.jsonl' to Hugging Face.
    - name: Upload updated dataset to HuggingFace
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        set -e
        python pipeline/push_to_hf.py

    # -------------------------------
    # 7. Auto Commit Changes
    # -------------------------------
    # Commits the changes to GitHub so the next hourly run sees the accumulated data.
    # At midnight, this commit effectively "shrinks" the file in the repo history.
    - name: Auto Commit Changes
      if: ${{ success() }}
      uses: stefanzweifel/git-auto-commit-action@v4
      with:
        commit_message: "Auto update: Career data sync [IST Hour: $(TZ='Asia/Kolkata' date +%H)]"
        file_pattern: |
          train.jsonl
          pipeline/train.jsonl
