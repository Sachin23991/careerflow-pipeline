name: Update Career Dataset

on:
  schedule:
    - cron: "0 * * * *"   # every hour (GitHub uses UTC)
  workflow_dispatch:

permissions:
  contents: write

jobs:
  pipeline:
    runs-on: ubuntu-latest

    steps:
    # -------------------------------
    # 1. Checkout repository
    # -------------------------------
    - name: Checkout repo
      uses: actions/checkout@v3
      with:
        fetch-depth: 0

    # -------------------------------
    # 2. Setup Python
    # -------------------------------
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    # -------------------------------
    # 3. Install Dependencies
    # -------------------------------
    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install requests feedparser huggingface_hub transformers beautifulsoup4 lxml

    # -------------------------------
    # 4. Run scraping + filtering + dataset builder
    # -------------------------------
    - name: Run scraping + filtering + dataset builder
      run: |
        set -e
        python pipeline/scrape.py
        python pipeline/filter.py
        python pipeline/dataset_builder.py

    # --------------------------------------------------------
    # 5. Merge or Overwrite FINAL train.jsonl
    # --------------------------------------------------------
    - name: Merge or Overwrite train.jsonl
      run: |
        set -e
        echo "ðŸ“ Checking whether to append or overwrite..."

        # Get hour in IST (Asia/Kolkata)
        HOUR_IN_IST=$(TZ='Asia/Kolkata' date +%H)
        echo "ðŸ•’ Current hour in IST: $HOUR_IN_IST"

        # MIDNIGHT (IST) â†’ OVERWRITE
        if [ "$HOUR_IN_IST" = "00" ]; then
          echo "ðŸŒ™ Midnight (IST) â†’ Overwriting main train.jsonl"
          cp pipeline/train.jsonl train.jsonl
        else
          echo "â³ Hourly (IST) â†’ Append ONLY new unique items"
          python pipeline/dataset_merge.py
        fi

    # -------------------------------
    # 5.5 Debug: show dataset info before upload (optional but useful)
    # -------------------------------
    - name: Debug: Show dataset info before upload
      run: |
        echo "ðŸ“¦ Files in root:"
        ls -lh
        echo "ðŸ“¦ Files in pipeline/:"
        ls -lh pipeline || true

        echo "ðŸ”Ž Head of train.jsonl:"
        head -n 10 train.jsonl || echo "train.jsonl missing or empty"

        echo "ðŸ”Ž Head of pipeline/train.jsonl:"
        head -n 10 pipeline/train.jsonl || echo "pipeline/train.jsonl missing or empty"

    # -------------------------------
    # 6. Upload final dataset to HuggingFace
    # -------------------------------
    - name: Upload updated dataset to HuggingFace
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        set -e
        python pipeline/push_to_hf.py

    # --------------------------------------------------------
    # 7. CLEAR DATA LOCALLY (prevent GitHub 100 MB limit)
    #    Only if everything above succeeded
    # --------------------------------------------------------
    - name: Clear train.jsonl after upload
      if: ${{ success() }}
      run: |
        echo "" > train.jsonl
        echo "" > pipeline/train.jsonl
        echo "ðŸ§¹ Cleared local files to prevent size growth."

    # -------------------------------
    # 8. Auto Commit Changes
    # -------------------------------
    - name: Auto Commit Changes
      if: ${{ success() }}
      uses: stefanzweifel/git-auto-commit-action@v4
      with:
        commit_message: "Auto update: New scraped career data"
        file_pattern: |
          train.jsonl
          pipeline/train.jsonl
