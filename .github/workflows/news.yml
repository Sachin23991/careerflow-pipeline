name: Update Career Dataset

on:
  schedule:
    - cron: "0 * * * *"   # every hour
  workflow_dispatch:

permissions:
  contents: write

jobs:
  pipeline:
    runs-on: ubuntu-latest

    steps:

    # -------------------------------
    # 1. Checkout repository
    # -------------------------------
    - name: Checkout repo
      uses: actions/checkout@v3

    # -------------------------------
    # 2. Setup Python
    # -------------------------------
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    # -------------------------------
    # 3. Install Dependencies
    # -------------------------------
    - name: Install dependencies
      run: |
        pip install requests feedparser huggingface_hub transformers beautifulsoup4 lxml

    # -------------------------------
    # 4. Run scraping + filtering + dataset builder
    # -------------------------------
    - name: Run scraping + filtering + dataset builder
      run: |
        python pipeline/scrape.py
        python pipeline/filter.py
        python pipeline/dataset_builder.py

    # --------------------------------------------------------
    # 5. Merge or Overwrite FINAL train.jsonl
    # --------------------------------------------------------
    - name: Merge or Overwrite train.jsonl
      run: |
        echo "ðŸ“ Checking whether to append or overwrite..."

        # MIDNIGHT BEHAVIOR â†’ OVERWRITE
        if [ "$(date +%H)" = "00" ]; then
          echo "ðŸŒ™ Midnight â†’ Overwriting main train.jsonl"
          cp pipeline/train.jsonl train.jsonl
        else
          echo "â³ Hourly â†’ Append ONLY new unique items"
          python pipeline/dataset_merge.py
        fi

    # -------------------------------
    # 6. Upload final dataset to HuggingFace
    # -------------------------------
    - name: Upload updated dataset to HuggingFace
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        python pipeline/push_to_hf.py

    # --------------------------------------------------------
    # 7. CLEAR DATA LOCALLY (prevent GitHub 100 MB limit)
    # --------------------------------------------------------
    - name: Clear train.jsonl after upload
      run: |
        echo "" > train.jsonl
        echo "" > pipeline/train.jsonl
        echo "ðŸ§¹ Cleared local files to prevent size growth."

    # -------------------------------
    # 8. Auto Commit Changes
    # -------------------------------
    - name: Auto Commit Changes
      uses: stefanzweifel/git-auto-commit-action@v4
      with:
        commit_message: "Auto update: New scraped career data"
